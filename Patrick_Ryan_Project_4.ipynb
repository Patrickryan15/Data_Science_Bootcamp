{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function Declarations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    try:\n",
    "        df = pd.read_csv(data)\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError, pd.errors.ParserError) as e:\n",
    "        logging.error(f\"Error: {str(e)}.\")\n",
    "        exit(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_types(df, unique_value_threshold=10):\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for column in df.columns:\n",
    "        # If the column is of object type, it's categorical\n",
    "        if df[column].dtype == 'object':\n",
    "            categorical_columns.append(column)\n",
    "        else:\n",
    "            # If the number of unique values is less than the threshold, consider it categorical\n",
    "            if df[column].nunique() <= unique_value_threshold:\n",
    "                categorical_columns.append(column)\n",
    "            else:\n",
    "                numerical_columns.append(column)\n",
    "    return numerical_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(data, target_column, top_n=10):\n",
    "    # Select only numeric columns for correlation\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    correlation_with_target = numeric_data.corr()[target_column].sort_values(ascending=False)\n",
    "    print(f\"Top {top_n} Features Correlated with {target_column}:\\n\")\n",
    "    print(correlation_with_target.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_encode(df, numerical_cols, categorical_cols):\n",
    "    # Define the transformations for numerical and categorical columns\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    # Create the column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    # Fit and transform the data\n",
    "    df_transformed = preprocessor.fit_transform(df)\n",
    "    # Get feature names after one-hot encoding\n",
    "    one_hot_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "    all_feature_names = numerical_cols + list(one_hot_feature_names)\n",
    "    # Create a new DataFrame with the transformed data\n",
    "    df_transformed = pd.DataFrame(df_transformed, columns=all_feature_names)\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(df, patient_identifier_col, target_col, window_col):\n",
    "    # Sort the dataframe by patient visit identifier and window\n",
    "    df_sorted = df.sort_values(by=[patient_identifier_col, window_col])\n",
    "    # Group by patient visit identifier\n",
    "    grouped = df_sorted.groupby(patient_identifier_col)\n",
    "    X = []  # To store sequences\n",
    "    y = []  # To store labels (ICU admission status)\n",
    "    for _, group in grouped:\n",
    "        # Find the first instance of ICU admission\n",
    "        first_icu_admission = group[target_col].cumsum().shift(fill_value=0).eq(1)\n",
    "        # Exclude data after the first ICU admission\n",
    "        group = group[~first_icu_admission]\n",
    "        # Drop columns that are not features (like identifiers and target)\n",
    "        features = group.drop(columns=[patient_identifier_col, target_col, window_col])\n",
    "        # Append the sequence of features to X\n",
    "        X.append(features.values)\n",
    "        # Append the label (ICU admission status) to y\n",
    "        y.append(group[target_col].max())  # max() ensures we capture if the patient was ever admitted to ICU\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_cnn_model(input_shape, num_classes, filters=64, kernel_size=3, lstm_units=100, dropout_rate=0.5, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    # Convolutional layers\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    # LSTM layer\n",
    "    model.add(LSTM(lstm_units, activation='tanh'))\n",
    "    # Flatten the output of the LSTM layers\n",
    "    model.add(Flatten())\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # Dense layer for classification\n",
    "    model.add(Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Script Start</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
