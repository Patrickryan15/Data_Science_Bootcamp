{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras_tuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function Declarations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    try:\n",
    "        df = pd.read_csv(data)\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError, pd.errors.ParserError) as e:\n",
    "        logging.error(f\"Error: {str(e)}.\")\n",
    "        exit(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_types(df, unique_value_threshold=10):\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for column in df.columns:\n",
    "        # If the column is of object type, it's categorical\n",
    "        if df[column].dtype == 'object':\n",
    "            categorical_columns.append(column)\n",
    "        else:\n",
    "            # If the number of unique values is less than the threshold, consider it categorical\n",
    "            if df[column].nunique() <= unique_value_threshold:\n",
    "                categorical_columns.append(column)\n",
    "            else:\n",
    "                numerical_columns.append(column)\n",
    "    return numerical_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_encode(df, numerical_cols, categorical_cols):\n",
    "    # Define the transformations for numerical and categorical columns\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    # Create the column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    # Fit and transform the data\n",
    "    df_transformed = preprocessor.fit_transform(df)\n",
    "    # Get feature names after one-hot encoding\n",
    "    one_hot_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "    all_feature_names = numerical_cols + list(one_hot_feature_names)\n",
    "    # Create a new DataFrame with the transformed data\n",
    "    df_transformed = pd.DataFrame(df_transformed, columns=all_feature_names)\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    # Create missingness indicators\n",
    "    missing_indicators = df.isna().astype(int).add_suffix('_missing')\n",
    "    # Apply forward fill imputation\n",
    "    df_imputed = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    # Combine imputed data with missingness indicators\n",
    "    df_combined = pd.concat([df_imputed, missing_indicators], axis=1)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(df, target_column='ICU', threshold=0.01, additional_cols=['PATIENT_VISIT_IDENTIFIER', 'WINDOW']):\n",
    "    # Handling missing values - consider modifying this as per your dataset's requirements\n",
    "    df = df.dropna()\n",
    "    # Separating the target variable\n",
    "    y = df[target_column]\n",
    "    # Separating the additional columns\n",
    "    additional_data = df[additional_cols]\n",
    "    # Dropping the target and additional columns from the main DataFrame\n",
    "    df = df.drop(additional_cols + [target_column], axis=1, errors='ignore')\n",
    "    # Encoding categorical variables if any\n",
    "    df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "    # Using RandomForestClassifier for feature importance\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(df_encoded, y)\n",
    "    # Selecting features based on importance\n",
    "    sfm = SelectFromModel(model, threshold=threshold)\n",
    "    sfm.fit(df_encoded, y)\n",
    "    # Getting the selected feature names\n",
    "    feature_names = df_encoded.columns[sfm.get_support()]\n",
    "    # Creating a DataFrame with selected features and additional columns\n",
    "    df_selected = df_encoded[feature_names].join(additional_data).join(y)\n",
    "    return df_selected, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(df, patient_identifier_col, target_col, window_col):\n",
    "    # Sort the dataframe by patient visit identifier and window\n",
    "    df_sorted = df.sort_values(by=[patient_identifier_col, window_col])\n",
    "    # Group by patient visit identifier\n",
    "    grouped = df_sorted.groupby(patient_identifier_col)\n",
    "    X = []  # To store sequences\n",
    "    y = []  # To store labels (ICU admission status)\n",
    "    for _, group in grouped:\n",
    "        # Find the first instance of ICU admission\n",
    "        first_icu_admission = group[target_col].cumsum().shift(fill_value=0).eq(1)\n",
    "        # Exclude data after the first ICU admission\n",
    "        group = group[~first_icu_admission]\n",
    "        # Drop columns that are not features (like identifiers and target)\n",
    "        features = group.drop(columns=[patient_identifier_col, target_col, window_col])\n",
    "        # Append the sequence of features to X\n",
    "        X.append(features.values)\n",
    "        # Append the label (ICU admission status) to y\n",
    "        y.append(group[target_col].max())  # max() ensures we capture if the patient was ever admitted to ICU\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_cnn_model(input_shape, num_classes, filters, kernel_size, lstm_units, dense_units, dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "    # Add MaxPooling1D only if the input is large enough\n",
    "    if input_shape[0] > kernel_size:\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(lstm_units, activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(input_shape, num_classes, X_train, y_train, X_val, y_val):\n",
    "    def model_builder(hp):\n",
    "        hp_filters = hp.Int('filters', min_value=16, max_value=64, step=16)\n",
    "        hp_kernel_size = hp.Choice('kernel_size', values=[3, 5])\n",
    "        hp_lstm_units = hp.Int('lstm_units', min_value=30, max_value=100, step=10)\n",
    "        hp_dense_units = hp.Int('dense_units', min_value=16, max_value=64, step=16)\n",
    "        hp_dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "        hp_learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "\n",
    "        return create_lstm_cnn_model(input_shape, num_classes, hp_filters, hp_kernel_size, hp_lstm_units, hp_dense_units, hp_dropout_rate, hp_learning_rate)\n",
    "\n",
    "    tuner = Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=50, \n",
    "        factor=3,\n",
    "        directory='my_dir',\n",
    "        project_name='lstm_cnn_tuning_hyperband')\n",
    "\n",
    "    tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
    "\n",
    "    return tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(X, y, input_shape, num_classes, best_hps, n_folds=5, epochs=50, batch_size=32):\n",
    "    # Define K-fold cross-validator\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    # K-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        # Split data\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        # Create and compile the model with the best hyperparameters\n",
    "        model = create_lstm_cnn_model(\n",
    "            input_shape=input_shape, \n",
    "            num_classes=num_classes, \n",
    "            filters=best_hps.get('filters'), \n",
    "            kernel_size=best_hps.get('kernel_size'), \n",
    "            lstm_units=best_hps.get('lstm_units'), \n",
    "            dense_units=best_hps.get('dense_units'), \n",
    "            dropout_rate=best_hps.get('dropout_rate'), \n",
    "            learning_rate=best_hps.get('learning_rate'))\n",
    "        # EarlyStopping callback\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True)\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping_callback])\n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_val, y_val)\n",
    "        results.append(accuracy)\n",
    "\n",
    "    # Calculate average performance across all folds\n",
    "    average_performance = np.mean(results)\n",
    "    return average_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, input_shape, num_classes, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    # Create the model\n",
    "    model = create_lstm_cnn_model(input_shape, num_classes, learning_rate=learning_rate)\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                        epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1) if y_pred.shape[1] > 1 else (y_pred > 0.5).astype('int32')\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading the dataset...\")\n",
    "    df = load_data(\"COVID-Full.csv\")\n",
    "    print(\"Identifying data types...\")\n",
    "    numerical_cols, categorical_cols = identify_data_types(df)\n",
    "    print(\"Normalizing and encoding data...\")\n",
    "    df_encoded = normalize_and_encode(df, numerical_cols, categorical_cols)\n",
    "    print(\"Handling missing values...\")\n",
    "    df_no_missing = handle_missing_values(df_encoded)\n",
    "        # Check if 'PATIENT_VISIT_IDENTIFIER', 'ICU', 'WINDOW' are in df_no_missing\n",
    "    if 'PATIENT_VISIT_IDENTIFIER' not in df_no_missing.columns:\n",
    "        df_no_missing['PATIENT_VISIT_IDENTIFIER'] = df['PATIENT_VISIT_IDENTIFIER']\n",
    "    if 'ICU' not in df_no_missing.columns:\n",
    "        df_no_missing['ICU'] = df['ICU']\n",
    "    if 'WINDOW' not in df_no_missing.columns:\n",
    "        df_no_missing['WINDOW'] = df['WINDOW']\n",
    "    print(\"Preparing sequence data...\")\n",
    "    X, y = prepare_sequence_data(df_no_missing, 'PATIENT_VISIT_IDENTIFIER', 'ICU', 'WINDOW')\n",
    "    print(\"Converting data to numpy arrays...\")\n",
    "    X_array = np.array(X, dtype=object)\n",
    "    y_array = np.array(y)\n",
    "    print(\"Padding sequences...\")\n",
    "    max_sequence_length = max(len(sequence) for sequence in X_array)\n",
    "    feature_size = X_array[0].shape[1]\n",
    "    X_padded = np.array([np.pad(sequence, ((0, max_sequence_length - len(sequence)), (0, 0)), mode='constant', constant_values=0) for sequence in X_array])\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_padded, y_array, test_size=0.2, random_state=42)\n",
    "    print(\"Determining model parameters...\")\n",
    "    input_shape = (max_sequence_length, feature_size)\n",
    "    num_classes = 1\n",
    "    print(\"Performing K-Fold Cross-Validation...\")\n",
    "    average_accuracy = cross_validate_model(X_padded, y_array, input_shape, num_classes, n_folds=5, epochs=10, batch_size=32, learning_rate=0.001)\n",
    "    print(f\"Average Accuracy across folds: {average_accuracy}\")\n",
    "    print(\"Training the model...\")\n",
    "    model, history = train_model(X_train, y_train, X_val, y_val, input_shape, num_classes)\n",
    "    print(\"Evaluating the model...\")\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "    print(\"Process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Script Start</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
