{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"WordSection1\">\n",
    "\n",
    "<h1>SCRIPT INFO </h1>\n",
    "\n",
    "<table class=\"MsoTableGrid\" border=\"1\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-collapse:collapse;border:none;mso-border-alt:solid windowtext .5pt;\n",
    " mso-yfti-tbllook:1184;mso-padding-alt:0in 5.4pt 0in 5.4pt\">\n",
    " <tbody><tr style=\"mso-yfti-irow:0;mso-yfti-firstrow:yes\">\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.85pt;border:solid windowtext 1.0pt;\n",
    "  mso-border-alt:solid windowtext .5pt;background:#B4C6E7;mso-background-themecolor:\n",
    "  accent1;mso-background-themetint:102;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\"><b><span style=\"font-size:14.0pt;color:black;mso-color-alt:windowtext\">Project</span></b><b><span style=\"font-size:14.0pt\"><o:p></o:p></span></b></p>\n",
    "  </td>\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.85pt;border:solid windowtext 1.0pt;\n",
    "  border-left:none;mso-border-left-alt:solid windowtext .5pt;mso-border-alt:\n",
    "  solid windowtext .5pt;background:#B4C6E7;mso-background-themecolor:accent1;\n",
    "  mso-background-themetint:102;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\"><b><span style=\"font-size:14.0pt;color:black;mso-color-alt:windowtext\">Developer</span></b><b><span style=\"font-size:14.0pt\"><o:p></o:p></span></b></p>\n",
    "  </td>\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.9pt;border:solid windowtext 1.0pt;\n",
    "  border-left:none;mso-border-left-alt:solid windowtext .5pt;mso-border-alt:\n",
    "  solid windowtext .5pt;background:#B4C6E7;mso-background-themecolor:accent1;\n",
    "  mso-background-themetint:102;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\"><b><span style=\"font-size:14.0pt;color:black;mso-color-alt:windowtext\">Tools</span></b><b><span style=\"font-size:14.0pt\"><o:p></o:p></span></b></p>\n",
    "  </td>\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.9pt;border:solid windowtext 1.0pt;\n",
    "  border-left:none;mso-border-left-alt:solid windowtext .5pt;mso-border-alt:\n",
    "  solid windowtext .5pt;background:#B4C6E7;mso-background-themecolor:accent1;\n",
    "  mso-background-themetint:102;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\"><b><span style=\"font-size:14.0pt;color:black;mso-color-alt:windowtext\">Version</span></b><b><span style=\"font-size:14.0pt\"><o:p></o:p></span></b></p>\n",
    "  </td>\n",
    " </tr>\n",
    " <tr style=\"mso-yfti-irow:1\">\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.85pt;border:solid windowtext 1.0pt;\n",
    "  border-top:none;mso-border-top-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;\n",
    "  padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\">Bootcamp Project 2</p>\n",
    "  </td>\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.85pt;border-top:none;border-left:\n",
    "  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;\n",
    "  mso-border-top-alt:solid windowtext .5pt;mso-border-left-alt:solid windowtext .5pt;\n",
    "  mso-border-alt:solid windowtext .5pt;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\">Patrick Ryan</p>\n",
    "  </td>\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.9pt;border-top:none;border-left:\n",
    "  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;\n",
    "  mso-border-top-alt:solid windowtext .5pt;mso-border-left-alt:solid windowtext .5pt;\n",
    "  mso-border-alt:solid windowtext .5pt;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\">Python 3.12</p>\n",
    "  </td>\n",
    "  <td width=\"156\" valign=\"top\" style=\"width:116.9pt;border-top:none;border-left:\n",
    "  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;\n",
    "  mso-border-top-alt:solid windowtext .5pt;mso-border-left-alt:solid windowtext .5pt;\n",
    "  mso-border-alt:solid windowtext .5pt;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\">0.1</p>\n",
    "  </td>\n",
    " </tr>\n",
    " <tr style=\"mso-yfti-irow:2\">\n",
    "  <td width=\"623\" colspan=\"4\" valign=\"top\" style=\"width:467.5pt;border:solid windowtext 1.0pt;\n",
    "  border-top:none;mso-border-top-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;\n",
    "  background:#B4C6E7;mso-background-themecolor:accent1;mso-background-themetint:\n",
    "  102;padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\" align=\"center\" style=\"text-align:center\"><b><span style=\"font-size:14.0pt;color:black;mso-color-alt:windowtext\">Description</span><o:p></o:p></b></p>\n",
    "  </td>\n",
    " </tr>\n",
    " <tr style=\"mso-yfti-irow:3;mso-yfti-lastrow:yes\">\n",
    "  <td width=\"623\" colspan=\"4\" valign=\"top\" style=\"width:467.5pt;border:solid windowtext 1.0pt;\n",
    "  border-top:none;mso-border-top-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;\n",
    "  padding:0in 5.4pt 0in 5.4pt\">\n",
    "  <p class=\"MsoNormal\">Script for data processing and machine learning</p>\n",
    "  </td>\n",
    " </tr>\n",
    "</tbody></table>\n",
    "\n",
    "<p class=\"MsoNormal\"><o:p>&nbsp;</o:p></p>\n",
    "\n",
    "<p class=\"MsoNormal\"><o:p>&nbsp;</o:p></p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data manipulation and analysis\n",
    "import pandas as pd  # Pandas for handling data in tabular form\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "import matplotlib.pyplot as plt  # Matplotlib for data visualization\n",
    "import seaborn as sns  # Seaborn for statistical data visualization\n",
    "\n",
    "# Importing modules from scikit-learn for machine learning tasks\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_score, recall_score, f1_score,  # Metrics for model evaluation\n",
    "    ConfusionMatrixDisplay, confusion_matrix, roc_auc_score, roc_curve  # Confusion matrix and ROC curve\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder  # For data preprocessing\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score  # Grid search and cross-validation\n",
    "from sklearn.feature_selection import RFE  # Recursive Feature Elimination\n",
    "from imblearn.over_sampling import SMOTE  # Handling class imbalance using Synthetic Minority Over-sampling Technique\n",
    "import xgboost as xgb  # XGBoost library for gradient boosting\n",
    "from xgboost import XGBClassifier  # XGBoost classifier\n",
    "\n",
    "# Importing modules for logging and parallel processing\n",
    "import logging  # Logging for tracking and debugging\n",
    "from joblib import parallel_backend  # Joblib for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function Declarations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_percentile(value):\n",
    "    \"\"\"\n",
    "    Extracts the percentile value from the 'AGE_PERCENTIL' column.\n",
    "\n",
    "    Args:\n",
    "        value (str): The value from the 'AGE_PERCENTIL' column.\n",
    "\n",
    "    Returns:\n",
    "        int: The extracted percentile value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'Above' in value:\n",
    "            return 90\n",
    "        return int(value.replace('th', ''))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "\n",
    "def define_age_group(age):\n",
    "    \"\"\"\n",
    "    Defines the age group based on the provided age.\n",
    "\n",
    "    Args:\n",
    "        age (float): The age value.\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding age group ('child', 'young_adult', 'middle_adult', 'older_adult', 'elderly').\n",
    "    \"\"\"\n",
    "    if age <= 18:\n",
    "        return 'child'\n",
    "    elif 18 < age <= 35:\n",
    "        return 'young_adult'\n",
    "    elif 35 < age <= 50:\n",
    "        return 'middle_adult'\n",
    "    elif 50 < age <= 65:\n",
    "        return 'older_adult'\n",
    "    else:\n",
    "        return 'elderly'\n",
    "\n",
    "def handle_missing_values(df, numerical_cols, important_features=None):\n",
    "    \"\"\"\n",
    "    Handles missing values in the DataFrame by either removing rows with missing values\n",
    "    or imputing missing values based on the specified strategy.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing missing values.\n",
    "        numerical_cols (list): List of numerical columns for imputation.\n",
    "        important_features (list, optional): List of important features. If provided, rows with missing values for these features will be removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    logging.info(\"Handling missing values...\")\n",
    "    original_rows = df.shape[0]  # Total number of rows in the original DataFrame\n",
    "    \n",
    "    # Check if any important features have missing values\n",
    "    if important_features:\n",
    "        logging.info(\"Removing rows with missing values for important features...\")\n",
    "        missing_cols = [col for col in important_features if col in df.columns and df[col].isnull().any()]\n",
    "        if missing_cols:\n",
    "            # Remove rows with missing values for important features\n",
    "            df_cleaned = df.dropna(subset=missing_cols)\n",
    "            remaining_rows = df_cleaned.shape[0]  # Number of rows after cleaning\n",
    "            removed_rows = original_rows - remaining_rows\n",
    "            logging.info(f\"Removed {removed_rows} rows ({(removed_rows / original_rows) * 100:.2f}%) with missing values for important features.\")\n",
    "            logging.info(f\"Percentage of data remaining: {(remaining_rows / original_rows) * 100:.2f}%\")\n",
    "        else:\n",
    "            logging.info(\"No rows with missing values for important features. DataFrame remains unchanged.\")\n",
    "            df_cleaned = df.copy()\n",
    "    else:\n",
    "        logging.info(\"Removing all rows with missing values...\")\n",
    "        df_cleaned = df.dropna()\n",
    "        remaining_rows = df_cleaned.shape[0]  # Number of rows after cleaning\n",
    "        removed_rows = original_rows - remaining_rows\n",
    "        logging.info(f\"Removed {removed_rows} rows ({(removed_rows / original_rows) * 100:.2f}%) with missing values.\")\n",
    "        logging.info(f\"Percentage of data remaining: {(remaining_rows / original_rows) * 100:.2f}%\")\n",
    "\n",
    "    # Convert numerical_cols to pandas Index\n",
    "    numerical_cols = pd.Index(numerical_cols)\n",
    "\n",
    "    # Impute missing values for numerical columns\n",
    "    if not numerical_cols.empty:\n",
    "        imputer_numeric = SimpleImputer(strategy='median')\n",
    "        df_cleaned[numerical_cols] = imputer_numeric.fit_transform(df_cleaned[numerical_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns\n",
    "    for col in df_cleaned.columns:\n",
    "        if col not in numerical_cols:\n",
    "            df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "def handle_class_imbalance(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Handles class imbalance in the target variable using Synthetic Minority Over-sampling Technique (SMOTE).\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The input features of the training set.\n",
    "        y_train (pd.Series): The target variable of the training set.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the resampled features (X_resampled) and the corresponding resampled target variable (y_resampled).\n",
    "    \"\"\"\n",
    "    logging.info(\"Handling class imbalance...\")\n",
    "    print(\"Unique values in y_train before SMOTE:\", np.unique(y_train))\n",
    "    \n",
    "    # Check if labels are numeric\n",
    "    if np.issubdtype(y_train.dtype, np.number):\n",
    "        # Convert to binary labels based on a threshold\n",
    "        threshold = 0.5\n",
    "        y_train_binary = (y_train > threshold).astype(int)\n",
    "        # Use SMOTE to handle class imbalance\n",
    "        smote = SMOTE(sampling_strategy='auto', random_state=42)  # 'auto' adjusts the strategy based on the input data\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train_binary)\n",
    "        return X_resampled, y_resampled\n",
    "    else:\n",
    "        # If labels are not numeric, proceed with SMOTE without conversion\n",
    "        smote = SMOTE(sampling_strategy='auto', random_state=42)  # 'auto' adjusts the strategy based on the input data\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def feature_engineering(df_encoded):\n",
    "    \"\"\"\n",
    "    Performs feature engineering on the input DataFrame by creating a new categorical feature 'AGE_GROUP'\n",
    "    based on the 'AGE_ABOVE65' column.\n",
    "\n",
    "    Args:\n",
    "        df_encoded (pd.DataFrame): The input DataFrame containing encoded features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the additional 'AGE_GROUP' feature.\n",
    "    \"\"\"\n",
    "    logging.info(\"Performing feature engineering...\")\n",
    "\n",
    "    # Create a duplicate of the DataFrame to avoid modifying the original\n",
    "    df_encoded_copy = df_encoded.copy()\n",
    "\n",
    "    if df_encoded_copy.empty:\n",
    "        logging.warning(\"DataFrame is empty before feature engineering.\")\n",
    "        return df_encoded_copy\n",
    "\n",
    "    # Feature: Age Group (categorical)\n",
    "    df_encoded_copy['AGE_GROUP'] = pd.cut(df_encoded_copy['AGE_ABOVE65'],\n",
    "                                          bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                                          labels=False,\n",
    "                                          right=False)\n",
    "    logging.info(\"Feature engineering completed.\")\n",
    "    return df_encoded_copy\n",
    "\n",
    "\n",
    "def encode_categorical_variables(df_encoded, categorical_cols):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables in the DataFrame using one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        df_encoded (pd.DataFrame): The input DataFrame containing encoded features.\n",
    "        categorical_cols (list): List of categorical columns to be one-hot encoded.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with one-hot encoded categorical variables.\n",
    "    \"\"\"\n",
    "    logging.info(\"Encoding categorical variables...\")\n",
    "\n",
    "    if df_encoded.empty:\n",
    "        logging.warning(\"DataFrame is empty before encoding.\")\n",
    "\n",
    "    # Initialize the OneHotEncoder with drop='first' to avoid multicollinearity\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "    \n",
    "    # Perform one-hot encoding on the specified categorical columns\n",
    "    df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols)\n",
    "\n",
    "    print(\"After encoding:\")\n",
    "    print(df_encoded.head())\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def normalize_numerical_features(df_encoded, numerical_cols):\n",
    "    \"\"\"\n",
    "    Normalizes numerical features in the DataFrame using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        df_encoded (pd.DataFrame): The input DataFrame containing encoded features.\n",
    "        numerical_cols (list): List of numerical columns to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with normalized numerical features.\n",
    "    \"\"\"\n",
    "    logging.info(\"Normalizing numerical features...\")\n",
    "\n",
    "    if df_encoded.empty:\n",
    "        logging.warning(\"DataFrame is empty before normalization.\")\n",
    "\n",
    "    # Initialize the StandardScaler for normalization\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_encoded_copy = df_encoded.copy()\n",
    "\n",
    "    # Normalize numerical columns using StandardScaler\n",
    "    df_encoded_copy[numerical_cols] = scaler.fit_transform(df_encoded_copy[numerical_cols])\n",
    "\n",
    "    print(\"After normalization:\")\n",
    "    print(df_encoded_copy.head())\n",
    "    return df_encoded_copy\n",
    "\n",
    "\n",
    "def data_exploration(df_encoded):\n",
    "    \"\"\"\n",
    "    Performs exploratory data analysis (EDA) on the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_encoded (pd.DataFrame): The input DataFrame for exploratory data analysis.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Exploratory Data Analysis:\")\n",
    "\n",
    "    if df_encoded.empty:\n",
    "        logging.warning(\"DataFrame is empty.\")\n",
    "\n",
    "    # Display dataset information\n",
    "    print(\"Dataset Information:\")\n",
    "    print(df_encoded.info())\n",
    "\n",
    "    # Display summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df_encoded.describe())\n",
    "\n",
    "    # Plot the distribution of ICU Admission\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='ICU', data=df_encoded)\n",
    "    plt.title('Distribution of ICU Admission')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_grid_search(X_train, y_train, clf, param_grid, model_type):\n",
    "    \"\"\"\n",
    "    Performs grid search for hyperparameter tuning using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The input features of the training set.\n",
    "        y_train (pd.Series): The target variable of the training set.\n",
    "        clf (object): The classifier or model for which hyperparameter tuning is performed.\n",
    "        param_grid (dict): The grid of hyperparameters to search over.\n",
    "        model_type (str): The type of model being tuned (e.g., 'xgboost').\n",
    "\n",
    "    Returns:\n",
    "        object: The best estimator/model after grid search.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Performing grid search for {model_type} model...\")\n",
    "\n",
    "    # Initialize GridSearchCV with 5-fold cross-validation and accuracy as the scoring metric\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=6)\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    logging.info(f\"Best Parameters ({model_type}): {best_params}\")\n",
    "    \n",
    "    # Return the best estimator/model\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, model_type='xgboost', param_grid=None, selected_features=None):\n",
    "    \"\"\"\n",
    "    Trains a machine learning model using the specified model type, optionally with hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The input features of the training set.\n",
    "        y_train (pd.Series): The target variable of the training set.\n",
    "        model_type (str): The type of model to train (e.g., 'xgboost').\n",
    "        param_grid (dict, optional): The grid of hyperparameters for grid search (default: None).\n",
    "        selected_features (list, optional): The list of selected features to use in training (default: None).\n",
    "\n",
    "    Returns:\n",
    "        object: The trained machine learning model.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Training {model_type.capitalize()} model (this may take a while)...\")\n",
    "\n",
    "    # If selected features are specified, filter the training features\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "\n",
    "    # Initialize the XGBoost classifier with default parameters\n",
    "    if model_type == 'xgboost':\n",
    "        clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "        default_param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'min_child_weight': [1, 3, 5]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Supported types: 'xgboost'\")\n",
    "\n",
    "    # Use the provided hyperparameter grid or default grid\n",
    "    if param_grid is None:\n",
    "        param_grid = default_param_grid\n",
    "\n",
    "    # Remove 'min_samples_leaf' and 'min_samples_split' from param_grid\n",
    "    param_grid = {key: value for key, value in param_grid.items() if key not in ['min_samples_leaf', 'min_samples_split']}\n",
    "\n",
    "    # Perform grid search for hyperparameter tuning\n",
    "    best_estimator = perform_grid_search(X_train, y_train, clf, param_grid, model_type)\n",
    "    \n",
    "    return best_estimator\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a machine learning model using various metrics and visualizations.\n",
    "\n",
    "    Args:\n",
    "        model (object): The trained machine learning model.\n",
    "        X_test (pd.DataFrame): The input features of the test set.\n",
    "        y_test (pd.Series): The true target variable of the test set.\n",
    "        threshold (float, optional): The threshold for converting probabilities to binary predictions (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Evaluating model with threshold adjustment...\")\n",
    "\n",
    "    # Predict probabilities on the test set\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Convert probabilities to binary predictions based on the threshold\n",
    "    y_pred_adjusted = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    # Convert y_test to binary values (assuming it's continuous)\n",
    "    y_test_binary = (y_test > 0.5).astype(int)\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_binary, y_pred_adjusted))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test_binary, y_pred_adjusted))\n",
    "\n",
    "    # Print accuracy using the threshold-adjusted predictions\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_adjusted)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    precision = precision_score(y_test_binary, y_pred_adjusted)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "    recall = recall_score(y_test_binary, y_pred_adjusted)\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    f1 = f1_score(y_test_binary, y_pred_adjusted)\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    auc_roc = roc_auc_score(y_test_binary, y_pred_prob)\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_binary, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_binary, y_pred_adjusted)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def identify_column_type(df_current):\n",
    "    \"\"\"\n",
    "    Identifies the data type of each column in a DataFrame and categorizes them as either numeric or categorical.\n",
    "\n",
    "    Args:\n",
    "        df_current (pd.DataFrame): The DataFrame for which column types need to be identified.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists - categorical_cols and numerical_cols.\n",
    "               - categorical_cols (list): List of column names with categorical data.\n",
    "               - numerical_cols (list): List of column names with numerical data.\n",
    "    \"\"\"\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "\n",
    "    # Loop through each column in the DataFrame\n",
    "    for column in df_current.columns:\n",
    "        # Check if the data type of the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df_current[column]):\n",
    "            numerical_cols.append(column)\n",
    "        else:\n",
    "            categorical_cols.append(column)\n",
    "\n",
    "    # Return the identified categorical and numerical columns\n",
    "    return categorical_cols, numerical_cols\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Perform preprocessing on the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing raw data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame after applying various preprocessing steps.\n",
    "    \"\"\"\n",
    "    logging.info(\"Preprocessing data...\")\n",
    "\n",
    "    # Apply the function to 'AGE_PERCENTIL' column\n",
    "    df['AGE_PERCENTIL'] = df['AGE_PERCENTIL'].apply(extract_percentile)\n",
    "    \n",
    "    # Create the 'AGE_GROUP' feature\n",
    "    df['AGE_GROUP'] = df['AGE_PERCENTIL'].apply(define_age_group)\n",
    "    \n",
    "    # Encode the 'WINDOW' column using ordinal encoding\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[['0-2', '2-4', '4-6', '6-12', 'ABOVE_12']])\n",
    "    df[['WINDOW']] = ordinal_encoder.fit_transform(df[['WINDOW']])\n",
    "    \n",
    "    # Drop the 'AGE_PERCENTIL' column if no longer needed\n",
    "    df.drop('AGE_PERCENTIL', axis=1, inplace=True)\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_cols, numerical_cols = identify_column_type(df)\n",
    "    \n",
    "    # Handling Missing Values\n",
    "    df = handle_missing_values(df, numerical_cols)\n",
    "    \n",
    "    # Convert the 'ICU' column to categorical\n",
    "    df['ICU'] = df['ICU'].astype(int)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df_encoded = feature_engineering(df)\n",
    "    \n",
    "    # Identify and remove constant features\n",
    "    constant_features = df_encoded.columns[df_encoded.nunique() == 1]\n",
    "    df_encoded = df_encoded.drop(columns=constant_features)\n",
    "    \n",
    "    # Redefine categorical and numerical columns for the updated dataframe\n",
    "    categorical_cols_updated, numerical_cols_updated = identify_column_type(df_encoded)\n",
    "    \n",
    "    # Encoding Categorical Variables\n",
    "    df_encoded = encode_categorical_variables(df_encoded, categorical_cols_updated)\n",
    "    \n",
    "    # Normalizing Numerical Features\n",
    "    df_encoded = normalize_numerical_features(df_encoded, numerical_cols_updated)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def identify_important_features(df_encoded, n_features_to_select_range=range(1, 21), threshold=0.5):\n",
    "    \"\"\"\n",
    "    Identify important features using Recursive Feature Elimination (RFE).\n",
    "\n",
    "    Args:\n",
    "        df_encoded (pd.DataFrame): The DataFrame containing encoded and preprocessed data.\n",
    "        n_features_to_select_range (range, optional): Range of the number of features to select in RFE. Defaults to range(1, 21).\n",
    "        threshold (float, optional): Threshold for converting labels to binary. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.Index: The selected important features based on RFE.\n",
    "    \"\"\"\n",
    "    logging.info(\"Identifying important features using Recursive Feature Elimination (RFE)...\")\n",
    "\n",
    "    # After handling class imbalance and before feature engineering, remove rows with missing values\n",
    "    categorical_cols, numerical_cols = identify_column_type(df_encoded)\n",
    "    df_no_missing = handle_missing_values(df_encoded, numerical_cols)\n",
    "\n",
    "    # Splitting the Data for initial training\n",
    "    X_task2_no_missing = df_no_missing.drop(['ICU'], axis=1)\n",
    "    y_task2_no_missing = df_no_missing['ICU']\n",
    "\n",
    "    # Convert to binary labels based on a threshold\n",
    "    y_task2_no_missing_binary = (y_task2_no_missing > threshold).astype(int)\n",
    "\n",
    "    X_train_no_missing_task2, X_test_no_missing_task2, y_train_no_missing_task2, y_test_no_missing_task2 = train_test_split(\n",
    "        X_task2_no_missing, y_task2_no_missing_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Handling Class Imbalance for the cleaned dataset\n",
    "    X_train_resampled_task2, y_train_resampled_task2 = handle_class_imbalance(\n",
    "        X_train_no_missing_task2, y_train_no_missing_task2)\n",
    "\n",
    "    # Initialize the XGBClassifier for RFE\n",
    "    clf_xgb_rfe = XGBClassifier(random_state=42)\n",
    "\n",
    "    # Initialize RFE with the XGBClassifier\n",
    "    rfe = RFE(estimator=clf_xgb_rfe, n_features_to_select=1)  # Start with 1 feature\n",
    "\n",
    "    # Evaluate performance for different numbers of features using parallelized cross-validation\n",
    "    cv_scores = []\n",
    "    with parallel_backend('loky', n_jobs=-1):  # Use all available CPUs\n",
    "        for n_features_to_select in n_features_to_select_range:\n",
    "            rfe.n_features_to_select = n_features_to_select\n",
    "            scores = cross_val_score(rfe, X_train_resampled_task2, y_train_resampled_task2, cv=5, scoring='accuracy')\n",
    "            cv_scores.append(scores.mean())\n",
    "\n",
    "    # Choose the number of features that maximizes the cross-validation score\n",
    "    optimal_n_features = n_features_to_select_range[cv_scores.index(max(cv_scores))]\n",
    "\n",
    "    # Fit RFE with the optimal number of features\n",
    "    rfe.n_features_to_select = optimal_n_features\n",
    "    rfe.fit(X_train_resampled_task2, y_train_resampled_task2)\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features_rfe = X_train_resampled_task2.columns[rfe.support_]\n",
    "\n",
    "    return selected_features_rfe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Script Start</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "data = \"COVID-Full.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(data)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Error: Data file '{data}' not found.\")\n",
    "    exit(1)\n",
    "except pd.errors.EmptyDataError:\n",
    "    logging.error(f\"Error: Data file '{data}' is empty.\")\n",
    "    exit(1)\n",
    "except pd.errors.ParserError:\n",
    "    logging.error(f\"Error: Unable to parse data from '{data}'. Check the file format.\")\n",
    "    exit(1)\n",
    "\n",
    "# Data Preprocessing\n",
    "df_encoded = data_preprocessing(df)\n",
    "\n",
    "# Identify important features\n",
    "important_features = identify_important_features(df_encoded)\n",
    "\n",
    "# Impute rows with missing values for important features from the original dataset\n",
    "df_cleaned = handle_missing_values(df_encoded, important_features)\n",
    "\n",
    "# Splitting the Data for updated training\n",
    "X_task2 = df_cleaned.drop(['ICU'], axis=1)\n",
    "y_task2 = df_cleaned['ICU']\n",
    "X_train_task2, X_test_task2, y_train_task2, y_test_task2 = train_test_split(\n",
    "    X_task2, y_task2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handling Class Imbalance for the updated dataset\n",
    "X_train_resampled_task2, y_train_resampled_task2 = handle_class_imbalance(X_train_task2, y_train_task2)\n",
    "\n",
    "# Train the XGBoost model on the updated data with selected features\n",
    "clf_xgb_tuned_task2 = train_model(\n",
    "    X_train_resampled_task2, y_train_resampled_task2, model_type='xgboost', selected_features=important_features)\n",
    "\n",
    "# Filter X_test_task2\n",
    "X_test_task2_filtered = X_test_task2[important_features]\n",
    "\n",
    "# Ensure y_test_task2 aligns with the filtered X_test_task2\n",
    "y_test_task2_filtered = y_test_task2\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(clf_xgb_tuned_task2, X_test_task2_filtered, y_test_task2_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
